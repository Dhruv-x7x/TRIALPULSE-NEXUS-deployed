{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# ðŸŽ¯ TrialPulse Nexus - Fine-Tuning\n",
        "\n",
        "Train your custom clinical trial AI model.\n",
        "\n",
        "**IMPORTANT:** Go to Runtime > Change runtime type > GPU (T4)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "install"
      },
      "source": [
        "# Step 1: Install dependencies (takes 2-3 minutes)\n",
        "!pip install unsloth\n",
        "!pip install --no-deps trl peft accelerate bitsandbytes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "load_model"
      },
      "source": [
        "# Step 2: Load the base model\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name='unsloth/llama-3.1-8b-bnb-4bit',\n",
        "    max_seq_length=2048,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "print('Model loaded!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "add_lora"
      },
      "source": [
        "# Step 3: Add LoRA adapters for efficient training\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'],\n",
        "    bias='none',\n",
        ")\n",
        "print('LoRA adapters added!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upload_data"
      },
      "source": [
        "# Step 4: Upload your training data\n",
        "# Click the folder icon on the left, then upload training_data_chat.jsonl\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "print('Data uploaded!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "load_data"
      },
      "source": [
        "# Step 5: Load the training data\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset('json', data_files='training_data_chat.jsonl', split='train')\n",
        "print(f'Loaded {len(dataset)} training examples')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "train"
      },
      "source": [
        "# Step 6: Train the model (takes ~20-30 minutes)\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field='messages',\n",
        "    max_seq_length=2048,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        num_train_epochs=3,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=10,\n",
        "        output_dir='outputs',\n",
        "        optim='adamw_8bit',\n",
        "    ),\n",
        ")\n",
        "\n",
        "print('Starting training...')\n",
        "trainer.train()\n",
        "print('Training complete!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "save"
      },
      "source": [
        "# Step 7: Save the model\n",
        "model.save_pretrained('trialpulse-nexus-v1')\n",
        "tokenizer.save_pretrained('trialpulse-nexus-v1')\n",
        "print('Model saved!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "download"
      },
      "source": [
        "# Step 8: Download your trained model\n",
        "!zip -r trialpulse_model.zip trialpulse-nexus-v1/\n",
        "from google.colab import files\n",
        "files.download('trialpulse_model.zip')\n",
        "print('Download started!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "test"
      },
      "source": [
        "# Step 9: Test your model\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "messages = [\n",
        "    {'role': 'system', 'content': 'You are a clinical trial data quality expert.'},\n",
        "    {'role': 'user', 'content': 'What is a DQI score and why is it important?'}\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(messages, return_tensors='pt').to('cuda')\n",
        "outputs = model.generate(inputs, max_new_tokens=200)\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}